# train_grpo.py - Medical VLM Reinforcement Learning (GRPO) Script
# 
# ğŸ¥ åŒ»ç–—è§†è§‰å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ è„šæœ¬ (GRPO)
# åŸºäº Unsloth å’Œ Qwen3-VL
#
# åŠŸèƒ½ï¼š
# 1. åŠ è½½ SFT åçš„æ¨¡å‹ä½œä¸ºåˆå§‹ç­–ç•¥
# 2. å®šä¹‰å¥–åŠ±å‡½æ•° (Reward Functions)ï¼š
#    - XML æ ¼å¼å¥–åŠ±ï¼šå¼ºåˆ¶æ¨¡å‹ä½¿ç”¨ <reasoning>...</reasoning> <answer>...</answer> æ ¼å¼
#    - é•¿åº¦å¥–åŠ±ï¼šé¼“åŠ±æ›´è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹
# 3. æ‰§è¡Œ GRPO è®­ç»ƒï¼šè®©æ¨¡å‹å­¦ä¼šâ€œå…ˆæ€è€ƒï¼Œå†å›ç­”â€
# 4. ä¿å­˜ RL åçš„æ¨¡å‹æƒé‡

import os
import re
import torch
from unsloth import FastVisionModel, is_bf16_supported
from trl import GRPOTrainer, GRPOConfig
from datasets import load_dataset
from transformers import AutoTokenizer

def main():
    print("ğŸš€ Starting Medical VLM GRPO Training...")

    # =================================================================
    # 1. é…ç½®ä¸æ¨¡å‹åŠ è½½
    # =================================================================
    # ç›´æ¥åŠ è½½ SFT åçš„ LoRA æ¨¡å‹ä½œä¸ºèµ·ç‚¹
    # å¦‚æœ lora_model å­˜åœ¨ï¼Œç›´æ¥åŠ è½½å®ƒï¼›å¦åˆ™åŠ è½½åŸºåº§
    if os.path.exists("lora_model"):
        print(f"Loading SFT model from: lora_model")
        MODEL_NAME = "lora_model" # Unsloth æ”¯æŒç›´æ¥åŠ è½½ LoRA ç›®å½•
    else:
        MODEL_NAME = "/root/autodl-tmp/models/unsloth/Qwen3-VL-8B-Instruct-bnb-4bit"
        print(f"'lora_model' not found! Using base model: {MODEL_NAME}")

    OUTPUT_DIR = "outputs_grpo"

    # åŠ è½½æ¨¡å‹
    model, tokenizer = FastVisionModel.from_pretrained(
        model_name=MODEL_NAME,
        load_in_4bit=True,
        device_map="auto",
        use_gradient_checkpointing="unsloth",
        local_files_only=True,
    )
    
    # é…ç½® LoRA (GRPO ä¹Ÿéœ€è¦ LoRA æ¥èŠ‚çœæ˜¾å­˜)
    print(" Configuring LoRA for GRPO...")
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»åŠ è½½äº† Adapter (ä» lora_model åŠ è½½æ—¶ä¼šè‡ªåŠ¨å¸¦ä¸Š)
    # å¦‚æœå·²ç»æœ‰ adapterï¼Œæˆ‘ä»¬åªéœ€è¦ç¡®ä¿å®ƒå¤„äºè®­ç»ƒæ¨¡å¼
    if hasattr(model, "peft_config") and len(model.peft_config) > 0:
        print("âœ… Model already has LoRA adapters. Enabling training mode...")
        FastVisionModel.for_training(model)
    else:
        # åªæœ‰å½“æ¨¡å‹æ˜¯çº¯åŸºåº§æ—¶ï¼Œæ‰éœ€è¦æ·»åŠ æ–°çš„ LoRA
        print("ğŸ†• Adding new LoRA adapters...")
        model = FastVisionModel.get_peft_model(
            model,
            finetune_vision_layers=False,
            finetune_language_layers=True,
            finetune_attention_modules=True,
            finetune_mlp_modules=True,
            r=16,
            lora_alpha=16,
            lora_dropout=0,
            bias="none",
            use_rslora=False,
        )

    # =================================================================
    # 2. å‡†å¤‡æ•°æ®é›†ä¸ Prompt æ ¼å¼
    # =================================================================
    print(" Loading dataset...")
    # è¿™é‡Œæˆ‘ä»¬å¤ç”¨ Radiology-mini æ•°æ®é›†ï¼Œä½†æˆ‘ä»¬éœ€è¦æ„é€ ä¸å¸¦ Answer çš„ Prompt
    # è®©æ¨¡å‹è‡ªå·±ç”Ÿæˆæ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆï¼Œç„¶åé€šè¿‡å¥–åŠ±å‡½æ•°æ¥è¯„ä¼°
    dataset = load_dataset("./data", split="train")

    # å®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œå¼ºåˆ¶è¦æ±‚ç‰¹å®šçš„è¾“å‡ºæ ¼å¼
    SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ”¾å°„ç§‘åŒ»ç”Ÿã€‚è¯·åˆ†æç»™å®šçš„åŒ»ç–—å›¾åƒã€‚
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä½ çš„è¯Šæ–­ç»“æœï¼Œå¹¶ä¸”åªè¾“å‡ºè¿™ä¸¤ä¸ªæ ‡ç­¾çš„å†…å®¹ï¼š
    
    <reasoning>
    åœ¨è¿™é‡Œå†™ä¸‹ä½ çš„è§‚å¯Ÿè¿‡ç¨‹ã€æ¨ç†é€»è¾‘å’Œåˆ†æç»†èŠ‚ã€‚
    </reasoning>
    <answer>
    åœ¨è¿™é‡Œç»™å‡ºæœ€ç»ˆçš„è¯Šæ–­ç»“è®ºã€‚
    </answer>
    """

    # GRPO éœ€è¦çš„æ•°æ®æ ¼å¼é€šå¸¸æ˜¯ prompt åˆ—
    def format_data(sample):
        # æ„é€ è¾“å…¥ Prompt
        messages = [
            {
                "role": "system", 
                "content": [{"type": "text", "text": SYSTEM_PROMPT}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": sample['image']},
                    {"type": "text", "text": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ã€‚"}
                ]
            }
        ]
        return {
            "prompt": messages,
            "ground_truth": sample['caption'] # 1) æ”¹å target -> ground_truth
        }

    # 1) æ”¹å target -> ground_truth, å¹¶å¢åŠ  num_proc=4 åŠ é€Ÿ
    dataset = dataset.map(format_data, remove_columns=["image", "caption", "image_id", "cui"], num_proc=4)

    # =================================================================
    # 3. å®šä¹‰å¥–åŠ±å‡½æ•° (Reward Functions)
    # =================================================================
    print("âš–ï¸ Defining Reward Functions...")

    # 1. æ ¼å¼å¥–åŠ±ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å« XML æ ‡ç­¾ï¼Œä¸”å†…å®¹å……å®
    def xml_format_reward(completions, **kwargs):
        rewards = []
        pattern = r"<reasoning>.*?</reasoning>\s*<answer>(.*?)</answer>"
        for completion in completions:
            text = completion[0]["content"] if isinstance(completion, list) else completion
            match = re.search(pattern, text, re.DOTALL)
            
            if match:
                # æ£€æŸ¥ <answer> å†…å®¹é•¿åº¦
                answer_content = match.group(1).strip()
                if len(answer_content) > 10: # è‡³å°‘æœ‰ 10 ä¸ªå­—ç¬¦
                    rewards.append(1.0)
                else:
                    rewards.append(0.5) # æ ¼å¼å¯¹ä½†å†…å®¹å¤ªçŸ­
            else:
                rewards.append(0.0)
        return rewards

    # 2. é•¿åº¦å¥–åŠ± (Length Reward)ï¼šé¼“åŠ±é€‚ä¸­é•¿åº¦çš„æ¨ç†
    # åˆ†æ®µ/é—¨æ§›å¼è®¾è®¡ï¼Œä¸¥å‰æ‰“å‡»è¿‡çŸ­å›å¤
    def length_reward(completions, **kwargs):
        rewards = []
        min_len = 80
        max_len = 250
        for completion in completions:
            text = completion[0]["content"] if isinstance(completion, list) else completion
            reasoning_match = re.search(r"<reasoning>(.*?)</reasoning>", text, re.DOTALL)
            if reasoning_match:
                reasoning_text = reasoning_match.group(1)
                length = len(reasoning_text)
                
                # åˆ†æ®µå¥–åŠ±é€»è¾‘
                if length < min_len:
                    # ä¸¥å‰æƒ©ç½šè¿‡çŸ­å›å¤ (å¦‚ 19 tokens)
                    rewards.append(-0.5)
                elif min_len <= length <= max_len:
                    # èˆ’é€‚åŒºç»™æ­£å¥–åŠ±
                    rewards.append(0.5)
                else: # length > max_len
                    # è¶…è¿‡ä¸Šé™ç»™è½»å¾®è´Ÿåˆ†ï¼Œé˜²æ­¢åºŸè¯
                    rewards.append(-0.1)
            else:
                rewards.append(0.0)
        return rewards
    
    # 3. æ­¥éª¤å¥–åŠ± (Step Reward)ï¼šé¼“åŠ±ç»“æ„åŒ–æ¨ç† (æ–°å¢)
    def step_reward(completions, **kwargs):
        rewards = []
        # æ£€æµ‹ "1.", "Step 1", "é¦–å…ˆ", "ç¬¬ä¸€" ç­‰æ­¥éª¤è¯
        step_patterns = [r"\d+\.", r"Step \d+", r"é¦–å…ˆ", r"å…¶æ¬¡", r"æœ€å", r"ç¬¬ä¸€", r"ç¬¬äºŒ"]
        for completion in completions:
            text = completion[0]["content"] if isinstance(completion, list) else completion
            reasoning_match = re.search(r"<reasoning>(.*?)</reasoning>", text, re.DOTALL)
            if reasoning_match:
                reasoning_text = reasoning_match.group(1)
                step_count = 0
                for p in step_patterns:
                    step_count += len(re.findall(p, reasoning_text))
                # æ¯ä¸ªæ­¥éª¤åŠ  0.1 åˆ†ï¼Œä¸Šé™ 0.5 åˆ†
                rewards.append(min(step_count * 0.1, 0.5))
            else:
                rewards.append(0.0)
        return rewards

    # 4. å‡†ç¡®ç‡å¥–åŠ± (Accuracy)ï¼šä¸»ç›®æ ‡ (æ”¹è¿›ç‰ˆ - å®ä½“å…³é”®è¯è¦†ç›–ç‡)
    # 1) ç­¾åä¿®æ”¹ï¼štarget -> ground_truth, å…¼å®¹ **kwargs
    def accuracy_reward(completions, ground_truth, **kwargs):
        rewards = []
        for completion, ref_answer in zip(completions, ground_truth):
            text = completion[0]["content"] if isinstance(completion, list) else completion
            # å°è¯•æå– <answer> å†…å®¹
            answer_match = re.search(r"<answer>(.*?)</answer>", text, re.DOTALL)
            
            # æå–é¢„æµ‹æ–‡æœ¬ï¼šå¦‚æœæœ‰æ ‡ç­¾å–æ ‡ç­¾å†…ï¼Œå¦åˆ™å–æœ€åä¸€æ®µï¼Œå†å¦åˆ™å–å…¨æ–‡
            if answer_match:
                pred_answer = answer_match.group(1).lower().strip()
            elif "<answer>" in text:
                pred_answer = text.split("<answer>")[-1].lower().strip()
            else:
                pred_answer = text.lower().strip()
            
            # é¢„å¤„ç†ï¼šç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
            pred_clean = re.sub(r'[^\w\s]', ' ', pred_answer)
            ref_clean = re.sub(r'[^\w\s]', ' ', ref_answer.lower())
            
            # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
            stop_words = {"the", "is", "a", "an", "of", "in", "on", "at", "and", "with", "to", "for", "it", "this", "that"}
            ref_tokens = set([w for w in ref_clean.split() if w not in stop_words and len(w) > 2])
            pred_tokens = set([w for w in pred_clean.split() if w not in stop_words and len(w) > 2])
            
            # åªè¦æœ‰ä»»ä½•é‡å å°±ç»™åŸºç¡€åˆ†ï¼Œé¿å…å…¨0
            intersection = ref_tokens.intersection(pred_tokens)
            
            if not ref_tokens:
                 # å‚è€ƒç­”æ¡ˆæ— æ•ˆæ—¶ï¼Œç»™ä¸€ä¸ªä¸­é—´åˆ†ä¿åº•
                 rewards.append(0.5)
                 continue

            if not intersection:
                 rewards.append(0.0)
            else:
                 # è®¡ç®—è¦†ç›–ç‡
                 recall = len(intersection) / len(ref_tokens)
                 
                 # é˜¶æ¢¯å¥–åŠ±è®¾è®¡ï¼šæ›´å¯†é›†çš„é˜¶æ¢¯ï¼Œç¡®ä¿æœ‰åˆ†å¯å¾—
                 if recall >= 0.9:
                     score = 2.0
                 elif recall >= 0.6:
                     score = 1.5
                 elif recall >= 0.3:
                     score = 1.0
                 else:
                     # åªè¦æœ‰å‘½ä¸­ (0 < recall < 0.3)ï¼Œå°±ç»™ 0.5 åˆ†
                     score = 0.5
                     
                 rewards.append(score)
        return rewards

    # =================================================================
    # 4. æ‰§è¡Œ GRPO è®­ç»ƒ
    # =================================================================
    print(" Starting GRPO training...")
    
    training_args = GRPOConfig(
        output_dir=OUTPUT_DIR,
        run_name="grpo_medical_vlm",
        learning_rate=5e-6,          # RL é€šå¸¸éœ€è¦æ›´ä½çš„å­¦ä¹ ç‡ (MD å»ºè®® 1e-6 ~ 1e-5)
        adam_beta1=0.9,
        adam_beta2=0.99,
        weight_decay=0.1,
        warmup_ratio=0.1,
        lr_scheduler_type="cosine",
        logging_steps=1,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        num_generations=4,           # æ¯ä¸ª prompt ç”Ÿæˆå¤šå°‘ä¸ªæ ·æœ¬ç”¨äºå¯¹æ¯” (Group Size)
        max_prompt_length=512,
        max_completion_length=384,   # å…è®¸ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ï¼Œä» 512 é™ä½åˆ° 384 ä»¥å‡å°‘æˆªæ–­æ¦‚ç‡
        max_steps=50,                # æ¼”ç¤ºç”¨
        save_steps=10,
        report_to="none",
        use_vllm=False,              # å¦‚æœæ˜¾å­˜å¤Ÿå¤§ä¸”å®‰è£…äº† vLLM å¯ä»¥å¼€å¯åŠ é€Ÿ
        bf16=is_bf16_supported(),
        
        # 1) è®­ç»ƒç›®æ ‡ä¸â€œå‚è€ƒç­–ç•¥ + KL çº¦æŸâ€
        # GRPO çš„æ ¸å¿ƒç¨³å®šå™¨é…ç½®
        beta=0.04,                   # KL coefficient (trl ä¸­é€šå¸¸å« beta)ï¼ŒMD å»ºè®® 0.01-0.1
        # clip_range=0.2,            # TRL çš„ GRPOConfig å¯èƒ½ä¸ç›´æ¥æš´éœ² clip_rangeï¼Œé€šå¸¸å†…ç½®å¤„ç†æˆ–é»˜è®¤å€¼
        # temperature=0.8,           # ç”Ÿæˆé‡‡æ ·æ¸©åº¦ï¼Œå½±å“æ¢ç´¢å¤šæ ·æ€§
    )

    trainer = GRPOTrainer(
        model=model,
        processing_class=tokenizer,
        reward_funcs=[xml_format_reward, length_reward, step_reward, accuracy_reward],
        args=training_args,
        train_dataset=dataset,
    )

    trainer.train()
    print(" GRPO Training completed.")

    # =================================================================
    # 5. ä¿å­˜æ¨¡å‹
    # =================================================================
    GRPO_OUTPUT_DIR = "grpo_model"
    print(f" Saving GRPO model to '{GRPO_OUTPUT_DIR}'...")
    model.save_pretrained(GRPO_OUTPUT_DIR)
    tokenizer.save_pretrained(GRPO_OUTPUT_DIR)
    print(" Model saved successfully!")

if __name__ == "__main__":
    main()
