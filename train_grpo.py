# train_grpo.py - Medical VLM Reinforcement Learning (GRPO) Script (Revised)
import os
import re
import torch
from unsloth import FastVisionModel, is_bf16_supported
from trl import GRPOTrainer, GRPOConfig
from datasets import load_dataset

# =========================
# 0) è®­ç»ƒç›®æ ‡å¼€å…³
# =========================
TARGET_LANG = "zh"   # "zh" æˆ– "en"
USE_ACCURACY_REWARD = False  #  GT æ˜¯è‹±æ–‡æ—¶ï¼šå¼ºçƒˆå»ºè®®å…ˆå…³æ‰ï¼Œå¦åˆ™æ¨¡å‹ä¼šè¢«æ‹‰å›è‹±æ–‡
ACC_WEIGHT = 0.3     # å¦‚æœä½ åšæŒå¼€ accuracy_rewardï¼ŒæŠŠæƒé‡å‹ä½ï¼ˆä¾‹å¦‚ 0.2~0.4ï¼‰

# å¥–åŠ±æƒé‡ï¼ˆå¯è°ƒï¼‰
W_FORMAT = 1.0
W_STOP   = 0.6
W_LEN    = 0.8
W_STEP   = 0.2
W_LANG   = 0.8
W_EARLY  = 1.2
W_ACC    = ACC_WEIGHT

# æœ€å°é•¿åº¦çº¦æŸ
MIN_REASONING_CHARS = 120
MIN_ANSWER_CHARS    = 30
MIN_TOTAL_CHARS     = 220    
MAX_REASONING_CHARS = 800    

STRICT_XML_PATTERN = re.compile(
    r"^\s*<reasoning>(?P<r>.*?)</reasoning>\s*<answer>(?P<a>.*?)</answer>\s*$",
    re.DOTALL
)

def _get_text(completion):
    # TRL/Unsloth é‡Œ completion å¯èƒ½æ˜¯ list[{"content": "..."}]
    if isinstance(completion, list):
        return completion[0].get("content", "")
    if isinstance(completion, dict):
        return completion.get("content", "")
    return str(completion)

def _parse_xml(text: str):
    m = STRICT_XML_PATTERN.search(text)
    if not m:
        return False, "", ""
    reasoning = m.group("r").strip()
    answer = m.group("a").strip()
    return True, reasoning, answer

def _count_zh_chars(s: str) -> int:
    return len(re.findall(r"[\u4e00-\u9fff]", s))

def _count_en_letters(s: str) -> int:
    return len(re.findall(r"[A-Za-z]", s))

# =========================
# 1) ä¸»ç¨‹åº
# =========================
def main():
    print(" Starting Medical VLM GRPO Training (Revised)...")

    # 1) æ¨¡å‹åŠ è½½
    if os.path.exists("lora_model"):
        print(" Loading SFT model from: lora_model")
        MODEL_NAME = "lora_model"
    else:
        MODEL_NAME = "/root/autodl-tmp/models/unsloth/Qwen3-VL-8B-Instruct-bnb-4bit"
        print(f"'lora_model' not found! Using base model: {MODEL_NAME}")

    OUTPUT_DIR = "outputs_grpo"

    model, tokenizer = FastVisionModel.from_pretrained(
        model_name=MODEL_NAME,
        load_in_4bit=True,
        device_map="auto",
        use_gradient_checkpointing="unsloth",
        local_files_only=True,
    )

    # LoRA ç¡®ä¿å¯è®­ç»ƒ
    if hasattr(model, "peft_config") and len(model.peft_config) > 0:
        print("âœ… Model already has LoRA adapters. Enabling training mode...")
        FastVisionModel.for_training(model)
    else:
        print("ğŸ†• Adding new LoRA adapters...")
        model = FastVisionModel.get_peft_model(
            model,
            finetune_vision_layers=False,
            finetune_language_layers=True,
            finetune_attention_modules=True,
            finetune_mlp_modules=True,
            r=16,
            lora_alpha=16,
            lora_dropout=0.05,   # ç¨å¾®åŠ ç‚¹ dropoutï¼ŒRL æ›´ç¨³ä¸€äº›
            bias="none",
            use_rslora=False,
        )

    # 2) æ•°æ®
    print(" Loading dataset...")
    dataset = load_dataset("./data", split="train")

    if TARGET_LANG == "zh":
        system_prompt = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ”¾å°„ç§‘åŒ»ç”Ÿã€‚è¯·åˆ†æç»™å®šçš„åŒ»ç–—å›¾åƒã€‚
è¦æ±‚ï¼š
1) å¿…é¡»ä½¿ç”¨ä¸­æ–‡ä½œç­”ï¼ˆåŒ»å­¦åè¯å¯ä¿ç•™è‹±æ–‡ç¼©å†™ï¼‰ã€‚
2) ä¸¥æ ¼åªè¾“å‡ºä»¥ä¸‹ä¸¤ä¸ªæ ‡ç­¾ï¼Œä¸”ä¸è¦è¾“å‡ºå¤šä½™æ–‡æœ¬ï¼š

<reasoning>
å†™ä¸‹è§‚å¯Ÿè¦ç‚¹ã€æ¨ç†è¿‡ç¨‹ã€ä¾æ®ï¼ˆä¸å°‘äº120å­—ï¼‰ã€‚
</reasoning>
<answer>
ç»™å‡ºæœ€ç»ˆè¯Šæ–­ç»“è®ºä¸å…³é”®å‘ç°ï¼ˆä¸å°‘äº30å­—ï¼‰ã€‚
</answer>
"""
        user_text = "è¯·ç”¨ä¸­æ–‡åˆ†æè¿™å¼ å›¾ç‰‡ã€‚"
    else:
        system_prompt = """
You are a professional radiologist. Analyze the given medical image.
Strictly output ONLY the following two tags:

<reasoning>
Write your observations and reasoning.
</reasoning>
<answer>
Write your final diagnosis.
</answer>
"""
        user_text = "Please analyze this image."

    def format_data(sample):
        messages = [
            {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            {"role": "user", "content": [{"type": "image", "image": sample["image"]},
                                         {"type": "text", "text": user_text}]},
        ]
        return {
            "prompt": messages,
            "ground_truth": sample["caption"],  # ä»ç„¶ä¿ç•™ï¼Œä»¥ä¾¿æœªæ¥åˆ‡æ¢
        }

    # å¤‡æ³¨ï¼šå¦‚æœimage æ˜¯ PIL/Arrow Imageï¼Œnum_proc å¤šè¿›ç¨‹æœ‰æ—¶ä¼šä¸ç¨³å®šï¼›ä¸ç¨³å°±æ”¹å› num_proc=1
    dataset = dataset.map(
        format_data,
        remove_columns=["image", "caption", "image_id", "cui"],
        num_proc=4
    )

    # =========================
    # 3) Reward functions
    # =========================
    def format_reward(completions, **kwargs):
        rewards = []
        for c in completions:
            text = _get_text(c)
            ok, r, a = _parse_xml(text)
            if not ok:
                rewards.append(0.0)
                continue

            # å¼ºçº¦æŸï¼šreasoning/answer éƒ½è¦å¤Ÿé•¿
            if len(r) < MIN_REASONING_CHARS or len(a) < MIN_ANSWER_CHARS:
                rewards.append(0.2)  # ç»“æ„å¯¹ä½†å¤ªçŸ­ï¼šåªç»™å¾ˆå°çš„ä¿åº•ï¼Œé¿å…æŠ•æœº
            else:
                rewards.append(1.0)
        return [x * W_FORMAT for x in rewards]

    def stop_reward(completions, **kwargs):
        rewards = []
        for c in completions:
            text = _get_text(c)
            ok, r, a = _parse_xml(text)
            if not ok:
                rewards.append(-0.2)  
                continue

            # åªæœ‰åœ¨æ»¡è¶³æœ€å°å†…å®¹åé—­åˆæ‰å¥–åŠ±ï¼›å¦åˆ™é—­åˆä¹Ÿæ‰£åˆ†
            if len(r) >= MIN_REASONING_CHARS and len(a) >= MIN_ANSWER_CHARS:
                rewards.append(0.5)
            else:
                rewards.append(-0.5)
        return [x * W_STOP for x in rewards]

    def length_reward(completions, **kwargs):
        rewards = []
        for c in completions:
            text = _get_text(c)
            ok, r, a = _parse_xml(text)
            if not ok:
                rewards.append(0.0)
                continue

            # reasoning èˆ’é€‚åŒºå¥–åŠ±ï¼›å¤ªçŸ­é‡ç½šï¼›å¤ªé•¿è½»ç½š
            L = len(r)
            if L < MIN_REASONING_CHARS:
                rewards.append(-1.0)
            elif L <= MAX_REASONING_CHARS:
                rewards.append(0.6)
            else:
                rewards.append(-0.2)
        return [x * W_LEN for x in rewards]

    def early_stop_penalty(completions, **kwargs):
        rewards = []
        for c in completions:
            text = _get_text(c)
            # ç›´æ¥å¯¹â€œæ•´ä½“æ–‡æœ¬é•¿åº¦â€åšç¡¬æƒ©ç½š
            if len(text.strip()) < MIN_TOTAL_CHARS:
                rewards.append(-1.0)
            else:
                rewards.append(0.0)
        return [x * W_EARLY for x in rewards]

    def step_reward(completions, **kwargs):
        rewards = []
        step_patterns = [r"\d+\.", r"Step\s*\d+", r"é¦–å…ˆ", r"å…¶æ¬¡", r"æœ€å", r"ç¬¬ä¸€", r"ç¬¬äºŒ", r"ç¬¬ä¸‰"]
        for c in completions:
            text = _get_text(c)
            ok, r, _ = _parse_xml(text)
            if not ok:
                rewards.append(0.0)
                continue
            step_count = 0
            for p in step_patterns:
                step_count += len(re.findall(p, r))
            rewards.append(min(step_count * 0.1, 0.3))
        return [x * W_STEP for x in rewards]

    def language_reward(completions, **kwargs):
        rewards = []
        if TARGET_LANG != "zh":
            return [0.0 for _ in completions]

        for c in completions:
            text = _get_text(c)
            ok, r, a = _parse_xml(text)
            if not ok:
                rewards.append(0.0)
                continue

            s = (r + "\n" + a)
            zh = _count_zh_chars(s)
            en = _count_en_letters(s)
            total = max(len(s), 1)

            zh_ratio = zh / total
            # å…¸å‹è‹±æ–‡å›ç­”ï¼šzh_ratio å¾ˆä½ã€en å¾ˆå¤š
            if zh_ratio >= 0.10:
                rewards.append(0.6)
            elif en >= 50 and zh_ratio < 0.03:
                rewards.append(-0.8)
            else:
                rewards.append(0.0)
        return [x * W_LANG for x in rewards]

    # ä½ åŸæ¥çš„ accuracy_rewardï¼ˆè‹±æ–‡ GTï¼‰ä¼šæŠŠæ¨¡å‹æ‹‰å›è‹±æ–‡ï¼Œé»˜è®¤å…ˆå…³é—­
    def accuracy_reward(completions, ground_truth, **kwargs):
        rewards = []
        stop_words = {"the", "is", "a", "an", "of", "in", "on", "at", "and", "with", "to", "for", "it", "this", "that"}
        for completion, ref_answer in zip(completions, ground_truth):
            text = _get_text(completion)
            ok, _, a = _parse_xml(text)
            pred = (a if ok else text).lower().strip()

            pred_clean = re.sub(r"[^\w\s]", " ", pred)
            ref_clean  = re.sub(r"[^\w\s]", " ", str(ref_answer).lower())

            ref_tokens  = set([w for w in ref_clean.split() if w not in stop_words and len(w) > 2])
            pred_tokens = set([w for w in pred_clean.split() if w not in stop_words and len(w) > 2])

            if not ref_tokens:
                rewards.append(0.2)
                continue
            inter = ref_tokens.intersection(pred_tokens)
            if not inter:
                rewards.append(0.0)
            else:
                recall = len(inter) / len(ref_tokens)
                if recall >= 0.6:
                    rewards.append(1.0)
                elif recall >= 0.3:
                    rewards.append(0.6)
                else:
                    rewards.append(0.3)
        return [x * W_ACC for x in rewards]

    reward_funcs = [format_reward, stop_reward, length_reward, early_stop_penalty, step_reward, language_reward]
    if USE_ACCURACY_REWARD:
        reward_funcs.append(accuracy_reward)

    # =========================
    # 4) GRPO Config
    # =========================
    training_args = GRPOConfig(
        output_dir=OUTPUT_DIR,
        run_name="grpo_medical_vlm_revised",
        learning_rate=2e-6,           
        adam_beta1=0.9,
        adam_beta2=0.99,
        weight_decay=0.05,
        warmup_ratio=0.05,
        lr_scheduler_type="cosine",
        logging_steps=1,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        num_generations=4,
        max_prompt_length=512,
        max_completion_length=512,    
        max_steps=60,                 # å…ˆè·‘æ›´ä¹…çœ‹è¶‹åŠ¿
        save_steps=20,
        report_to="none",
        use_vllm=False,
        bf16=is_bf16_supported(),
        beta=0.08,                    # KL çº¦æŸç¨å¾®åŠ å¼ºï¼Œå‹ä½å‘æ•£/æŠ•æœº
    )

    trainer = GRPOTrainer(
        model=model,
        processing_class=tokenizer,
        reward_funcs=reward_funcs,
        args=training_args,
        train_dataset=dataset,
    )

    trainer.train()

    GRPO_OUTPUT_DIR = "grpo_model"
    print(f" Saving GRPO model to '{GRPO_OUTPUT_DIR}'...")
    model.save_pretrained(GRPO_OUTPUT_DIR)
    tokenizer.save_pretrained(GRPO_OUTPUT_DIR)
    print("âœ… Done.")

if __name__ == "__main__":
    main()
