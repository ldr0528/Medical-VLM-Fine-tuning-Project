# train.py - Medical VLM Fine-tuning Script
# 
# ğŸ¥ åŒ»ç–—è§†è§‰å¤§æ¨¡å‹å¾®è°ƒè„šæœ¬
# åŸºäº Unsloth å’Œ Qwen2-VL
#
# åŠŸèƒ½ï¼š
# 1. åŠ è½½ 4-bit é‡åŒ–çš„ Qwen2-VL æ¨¡å‹
# 2. é…ç½® LoRA é€‚é…å™¨
# 3. åŠ è½½å¹¶å¤„ç†åŒ»ç–—æ•°æ®é›†
# 4. æ‰§è¡Œç›‘ç£å¾®è°ƒ (SFT)
# 5. ä¿å­˜å¾®è°ƒåçš„ LoRA æƒé‡

import os
import torch
from unsloth import FastVisionModel, is_bf16_supported
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
from transformers import TextStreamer

def main():
    print(" Starting Medical VLM Fine-tuning...")

    # =================================================================
    # 1. é…ç½®ä¸æ¨¡å‹åŠ è½½
    # =================================================================
    # æ¨¡å‹è·¯å¾„ (è¯·ä¿®æ”¹ä¸ºä½ çš„æœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹ ID)
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ 4-bit é‡åŒ–ç‰ˆæœ¬ä»¥èŠ‚çœæ˜¾å­˜
    MODEL_NAME = "/root/autodl-tmp/models/unsloth/Qwen3-VL-8B-Instruct-bnb-4bit"
    OUTPUT_DIR = "outputs"
    LORA_OUTPUT_DIR = "lora_model"

    print(f" Loading model from: {MODEL_NAME}")
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = FastVisionModel.from_pretrained(
        model_name=MODEL_NAME,
        load_in_4bit=True,
        device_map="auto",
        use_gradient_checkpointing="unsloth",
        local_files_only=True,
    )

    # =================================================================
    # 2. é…ç½® LoRA é€‚é…å™¨
    # =================================================================
    print(" Configuring LoRA adapter...")
    model = FastVisionModel.get_peft_model(
        model,
        finetune_vision_layers=False,  # ä¸å¾®è°ƒè§†è§‰å±‚
        finetune_language_layers=True, # é‡ç‚¹å¾®è°ƒè¯­è¨€å±‚
        finetune_attention_modules=True,
        finetune_mlp_modules=True,
        r=16,           # LoRA rank
        lora_alpha=16,  # Alpha å‚æ•°
        lora_dropout=0,
        bias="none",
        use_rslora=False,
        loftq_config=None,
    )

    # =================================================================
    # 3. æ•°æ®é›†åŠ è½½ä¸å¤„ç†
    # =================================================================
    print("Loading and processing dataset...")
    # åŠ è½½æœ¬åœ°æ•°æ®é›†
    # å‡è®¾ ./data ç›®å½•ä¸‹æœ‰æ­£ç¡®çš„ train æ•°æ®
    try:
        dataset = load_dataset("./data", split="train")
    except Exception as e:
        print(f" Error loading dataset: {e}")
        print("Please ensure your dataset is in the './data' directory.")
        return

    # å®šä¹‰ç³»ç»ŸæŒ‡ä»¤
    instruction = "ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè¯·å‡†ç¡®æè¿°ä½ åœ¨å›¾ç‰‡çœ‹åˆ°çš„å†…å®¹ã€‚"

    # æ•°æ®è½¬æ¢å‡½æ•°
    def convert_to_conversation(sample):
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": instruction},
                    {"type": "image", "image": sample['image']}
                ]
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": sample['caption']}
                ]
            }
        ]
        return {"messages": conversation}

    converted_dataset = [convert_to_conversation(sample) for sample in dataset]
    print(f" Processed {len(converted_dataset)} samples.")

    # =================================================================
    # 4. æ‰§è¡Œå¾®è°ƒ (Training)
    # =================================================================
    print("Starting training...")
    
    # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
    FastVisionModel.for_training(model)

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        data_collator=UnslothVisionDataCollator(model, tokenizer),
        train_dataset=converted_dataset,
        args=SFTConfig(
            per_device_train_batch_size=2,  # æ˜¾å­˜è¾ƒå°å¯è®¾ä¸º 1
            gradient_accumulation_steps=4,
            max_steps=30,                   # æ¼”ç¤ºç”¨æ­¥æ•°ï¼Œå®é™…è®­ç»ƒè¯·è°ƒå¤§ (e.g., 60-100)
            learning_rate=2e-4,
            warmup_steps=5,
            lr_scheduler_type="cosine",
            bf16=is_bf16_supported(),
            optim="adamw_8bit",
            weight_decay=0.01,
            seed=3407,
            logging_steps=1,
            output_dir=OUTPUT_DIR,
            report_to="none",
            remove_unused_columns=False,
            dataset_text_field="",
            dataset_kwargs={"skip_prepare_dataset": True},
            dataset_num_proc=4,
            max_seq_length=2048,
        )
    )

    trainer_stats = trainer.train()
    print("âœ… Training completed.")

    # =================================================================
    # 5. ä¿å­˜æ¨¡å‹
    # =================================================================
    print(f"ğŸ’¾ Saving LoRA model to '{LORA_OUTPUT_DIR}'...")
    model.save_pretrained(LORA_OUTPUT_DIR)
    tokenizer.save_pretrained(LORA_OUTPUT_DIR)
    print("âœ… Model saved successfully!")

    # =================================================================
    # 6. (å¯é€‰) ç®€å•çš„æ¨ç†æµ‹è¯•
    # =================================================================
    print("\nğŸ” Running post-training inference test...")
    FastVisionModel.for_inference(model)
    
    image = dataset[0]['image']
    test_messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": instruction},
                {"type": "image"}
            ]
        }
    ]
    
    input_text = tokenizer.apply_chat_template(test_messages, add_generation_prompt=True)
    inputs = tokenizer(
        image,
        input_text,
        add_special_tokens=False,
        return_tensors="pt"
    ).to("cuda")

    text_streamer = TextStreamer(tokenizer, skip_prompt=True)
    _ = model.generate(
        **inputs,
        streamer=text_streamer,
        max_new_tokens=128,
        use_cache=True,
        temperature=1.5,
        min_p=0.1,
    )

if __name__ == "__main__":
    main()
